{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKbSPFv59Rwd8fs933/nlb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/allalena/demo1/blob/main/PCA_Chalenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2d7ZrhCImp-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import pandas as pd\n",
        "import io\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import plotly.express as plotly\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import recall_score, confusion_matrix, precision_score, f1_score, accuracy_score, classification_report\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Read the csv file\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/allalena/demo1/main/CC_General.csv.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "puxBYoN9I5jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "8cW1sy1CJAE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check for missing values"
      ],
      "metadata": {
        "id": "efyUsmhBJEC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for missing values in the dataset\n",
        "missing_values = df.isnull().sum()\n",
        "missing_values_percentage = (missing_values / len(df)) * 100\n",
        "\n",
        "missing_values, missing_values_percentage"
      ],
      "metadata": {
        "id": "aBzBGqyUJCzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BHEuBysdJhMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Dropping the CUST_ID column as it's not useful for our analysis\n",
        "df.drop('CUST_ID', axis=1, inplace=True)\n",
        "\n",
        "# Imputing missing values\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "credit_card_data_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
        "\n",
        "# Normalizing the data\n",
        "scaler = StandardScaler()\n",
        "credit_card_data_scaled = pd.DataFrame(scaler.fit_transform(credit_card_data_imputed), columns=credit_card_data_imputed.columns)\n",
        "\n",
        "# Displaying the first few rows of the processed data\n",
        "credit_card_data_scaled.head()"
      ],
      "metadata": {
        "id": "3oWK3D2GJhcD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}